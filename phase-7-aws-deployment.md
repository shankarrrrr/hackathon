# PHASE 7: AWS DEPLOYMENT

> **Complete AWS deployment guide for the Pre-Delinquency Engine**  
> Replaces GCP services with AWS equivalents: ECS Fargate, MSK, RDS, ElastiCache, S3

## Overview

This guide provides a production-ready AWS deployment strategy for the Pre-Delinquency Engine, mapping the current local architecture to AWS managed services.

### Architecture Mapping

| Component | Local | AWS Service |
|-----------|-------|-------------|
| **API Server** | FastAPI + Uvicorn | ECS Fargate / App Runner |
| **Dashboard** | Streamlit | ECS Fargate / App Runner |
| **Message Broker** | Kafka | Amazon MSK (Managed Kafka) |
| **Database** | PostgreSQL | Amazon RDS PostgreSQL |
| **Cache** | Redis | Amazon ElastiCache Redis |
| **Model Storage** | Local files | Amazon S3 |
| **Container Registry** | Docker Hub | Amazon ECR |
| **Monitoring** | Logs | CloudWatch + X-Ray |
| **CI/CD** | Manual | CodePipeline + CodeBuild |

## AWS Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         INTERNET GATEWAY                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    APPLICATION LOAD BALANCER                         ‚îÇ
‚îÇ                    (Public Subnets)                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                               ‚îÇ
             ‚ñº                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ECS FARGATE CLUSTER    ‚îÇ    ‚îÇ   ECS FARGATE CLUSTER    ‚îÇ
‚îÇ   (Private Subnets)      ‚îÇ    ‚îÇ   (Private Subnets)      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  API Service             ‚îÇ    ‚îÇ  Dashboard Service       ‚îÇ
‚îÇ  - FastAPI               ‚îÇ    ‚îÇ  - Streamlit             ‚îÇ
‚îÇ  - Auto Scaling 2-10     ‚îÇ    ‚îÇ  - Auto Scaling 1-5      ‚îÇ
‚îÇ  - 2 vCPU, 4GB RAM       ‚îÇ    ‚îÇ  - 1 vCPU, 2GB RAM       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                  ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îÇ                                  ‚îÇ
         ‚ñº                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    AMAZON MSK (KAFKA)                                ‚îÇ
‚îÇ                    (Private Subnets)                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Topics:                                                             ‚îÇ
‚îÇ  - transactions-stream (3 partitions)                                ‚îÇ
‚îÇ  - predictions-stream (3 partitions)                                 ‚îÇ
‚îÇ  - interventions-stream (2 partitions)                               ‚îÇ
‚îÇ  - customer-updates (2 partitions)                                   ‚îÇ
‚îÇ  - dashboard-updates (1 partition)                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                   ‚îÇ                   ‚îÇ
         ‚ñº                   ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ECS FARGATE     ‚îÇ ‚îÇ  ECS FARGATE     ‚îÇ ‚îÇ  ECS FARGATE     ‚îÇ
‚îÇ  (Workers)       ‚îÇ ‚îÇ  (Workers)       ‚îÇ ‚îÇ  (Workers)       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Transaction      ‚îÇ ‚îÇ Feature          ‚îÇ ‚îÇ Intervention     ‚îÇ
‚îÇ Simulator        ‚îÇ ‚îÇ Processor        ‚îÇ ‚îÇ Worker           ‚îÇ
‚îÇ - Streams to MSK ‚îÇ ‚îÇ - Consumes MSK   ‚îÇ ‚îÇ - Consumes MSK   ‚îÇ
‚îÇ - 1 vCPU, 2GB    ‚îÇ ‚îÇ - Calls API      ‚îÇ ‚îÇ - Triggers       ‚îÇ
‚îÇ                  ‚îÇ ‚îÇ - 2 vCPU, 4GB    ‚îÇ ‚îÇ   interventions  ‚îÇ
‚îÇ                  ‚îÇ ‚îÇ - Auto Scale 2-5 ‚îÇ ‚îÇ - 1 vCPU, 2GB    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                    ‚îÇ                    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                    ‚îÇ                    ‚îÇ
         ‚ñº                    ‚ñº                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AMAZON RDS      ‚îÇ ‚îÇ  ELASTICACHE     ‚îÇ ‚îÇ  AMAZON S3       ‚îÇ
‚îÇ  PostgreSQL      ‚îÇ ‚îÇ  Redis           ‚îÇ ‚îÇ                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ - Multi-AZ       ‚îÇ ‚îÇ - Cluster Mode   ‚îÇ ‚îÇ - ML Models      ‚îÇ
‚îÇ - db.t3.medium   ‚îÇ ‚îÇ - cache.t3.micro ‚îÇ ‚îÇ - Training Data  ‚îÇ
‚îÇ - 100GB SSD      ‚îÇ ‚îÇ - 1GB memory     ‚îÇ ‚îÇ - Backups        ‚îÇ
‚îÇ - Auto Backup    ‚îÇ ‚îÇ - Auto Failover  ‚îÇ ‚îÇ - Logs           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                    ‚îÇ                    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    CLOUDWATCH + X-RAY                                ‚îÇ
‚îÇ  - Logs, Metrics, Alarms, Dashboards, Distributed Tracing           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Prerequisites

### 1. AWS Account Setup
```bash
# Install AWS CLI
pip install awscli

# Configure credentials
aws configure
# AWS Access Key ID: [your-key]
# AWS Secret Access Key: [your-secret]
# Default region: us-east-1
# Default output format: json
```

### 2. Install Required Tools
```bash
# Terraform for infrastructure
choco install terraform  # Windows
brew install terraform   # macOS

# Docker for building images
# Already installed from local setup

# AWS CDK (optional, alternative to Terraform)
npm install -g aws-cdk
```

### 3. Set Environment Variables
```bash
# Create .env.aws file
cat > .env.aws << EOF
AWS_REGION=us-east-1
AWS_ACCOUNT_ID=123456789012
PROJECT_NAME=pre-delinquency-engine
ENVIRONMENT=production
EOF
```

## Deployment Strategy

### Phase 1: Infrastructure Setup (Terraform)
### Phase 2: Container Images (ECR)
### Phase 3: Database Migration (RDS)
### Phase 4: Service Deployment (ECS)
### Phase 5: CI/CD Pipeline (CodePipeline)
### Phase 6: Monitoring & Alerts (CloudWatch)

---

## Phase 1: Infrastructure as Code (Terraform)

### Directory Structure
```
deployment/
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îú‚îÄ‚îÄ vpc.tf
‚îÇ   ‚îú‚îÄ‚îÄ rds.tf
‚îÇ   ‚îú‚îÄ‚îÄ msk.tf
‚îÇ   ‚îú‚îÄ‚îÄ elasticache.tf
‚îÇ   ‚îú‚îÄ‚îÄ ecs.tf
‚îÇ   ‚îú‚îÄ‚îÄ alb.tf
‚îÇ   ‚îú‚îÄ‚îÄ s3.tf
‚îÇ   ‚îú‚îÄ‚îÄ iam.tf
‚îÇ   ‚îî‚îÄ‚îÄ cloudwatch.tf
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh
‚îÇ   ‚îú‚îÄ‚îÄ build-images.sh
‚îÇ   ‚îî‚îÄ‚îÄ migrate-db.sh
‚îî‚îÄ‚îÄ cloudformation/  # Alternative to Terraform
    ‚îî‚îÄ‚îÄ stack.yaml
```

### Main Terraform Configuration

Create `deployment/terraform/main.tf`:

```hcl
terraform {
  required_version = ">= 1.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  
  backend "s3" {
    bucket         = "pre-delinquency-terraform-state"
    key            = "production/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "terraform-state-lock"
  }
}

provider "aws" {
  region = var.aws_region
  
  default_tags {
    tags = {
      Project     = "pre-delinquency-engine"
      Environment = var.environment
      ManagedBy   = "terraform"
    }
  }
}

# Data sources
data "aws_caller_identity" "current" {}
data "aws_availability_zones" "available" {
  state = "available"
}
```

### VPC Configuration

Create `deployment/terraform/vpc.tf`:

```hcl
module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"

  name = "${var.project_name}-vpc"
  cidr = "10.0.0.0/16"

  azs             = slice(data.aws_availability_zones.available.names, 0, 3)
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]

  enable_nat_gateway   = true
  single_nat_gateway   = var.environment == "dev" ? true : false
  enable_dns_hostnames = true
  enable_dns_support   = true

  # VPC Flow Logs
  enable_flow_log                      = true
  create_flow_log_cloudwatch_iam_role  = true
  create_flow_log_cloudwatch_log_group = true

  tags = {
    Name = "${var.project_name}-vpc"
  }
}

# Security Groups
resource "aws_security_group" "alb" {
  name_prefix = "${var.project_name}-alb-"
  description = "Security group for Application Load Balancer"
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_security_group" "ecs_tasks" {
  name_prefix = "${var.project_name}-ecs-tasks-"
  description = "Security group for ECS tasks"
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port       = 8000
    to_port         = 8000
    protocol        = "tcp"
    security_groups = [aws_security_group.alb.id]
  }

  ingress {
    from_port       = 8501
    to_port         = 8501
    protocol        = "tcp"
    security_groups = [aws_security_group.alb.id]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_security_group" "rds" {
  name_prefix = "${var.project_name}-rds-"
  description = "Security group for RDS PostgreSQL"
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port       = 5432
    to_port         = 5432
    protocol        = "tcp"
    security_groups = [aws_security_group.ecs_tasks.id]
  }
}

resource "aws_security_group" "elasticache" {
  name_prefix = "${var.project_name}-redis-"
  description = "Security group for ElastiCache Redis"
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port       = 6379
    to_port         = 6379
    protocol        = "tcp"
    security_groups = [aws_security_group.ecs_tasks.id]
  }
}

resource "aws_security_group" "msk" {
  name_prefix = "${var.project_name}-msk-"
  description = "Security group for Amazon MSK"
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port       = 9092
    to_port         = 9092
    protocol        = "tcp"
    security_groups = [aws_security_group.ecs_tasks.id]
  }

  ingress {
    from_port       = 9094
    to_port         = 9094
    protocol        = "tcp"
    security_groups = [aws_security_group.ecs_tasks.id]
  }
}
```

### RDS PostgreSQL Configuration

Create `deployment/terraform/rds.tf`:

```hcl
resource "aws_db_subnet_group" "main" {
  name       = "${var.project_name}-db-subnet"
  subnet_ids = module.vpc.private_subnets

  tags = {
    Name = "${var.project_name}-db-subnet"
  }
}

resource "aws_db_parameter_group" "postgres" {
  name   = "${var.project_name}-postgres15"
  family = "postgres15"

  parameter {
    name  = "log_connections"
    value = "1"
  }

  parameter {
    name  = "log_disconnections"
    value = "1"
  }

  parameter {
    name  = "log_duration"
    value = "1"
  }

  parameter {
    name  = "shared_preload_libraries"
    value = "pg_stat_statements"
  }
}

resource "aws_db_instance" "postgres" {
  identifier     = "${var.project_name}-db"
  engine         = "postgres"
  engine_version = "15.4"
  
  instance_class    = var.db_instance_class  # db.t3.medium
  allocated_storage = 100
  storage_type      = "gp3"
  storage_encrypted = true
  
  db_name  = "bank_data"
  username = "admin"
  password = random_password.db_password.result
  
  db_subnet_group_name   = aws_db_subnet_group.main.name
  vpc_security_group_ids = [aws_security_group.rds.id]
  parameter_group_name   = aws_db_parameter_group.postgres.name
  
  # High Availability
  multi_az               = var.environment == "production" ? true : false
  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "mon:04:00-mon:05:00"
  
  # Performance Insights
  enabled_cloudwatch_logs_exports = ["postgresql", "upgrade"]
  performance_insights_enabled    = true
  performance_insights_retention_period = 7
  
  # Protection
  deletion_protection = var.environment == "production" ? true : false
  skip_final_snapshot = var.environment != "production"
  final_snapshot_identifier = var.environment == "production" ? "${var.project_name}-final-snapshot" : null
  
  tags = {
    Name = "${var.project_name}-db"
  }
}

resource "random_password" "db_password" {
  length  = 32
  special = true
}

resource "aws_secretsmanager_secret" "db_password" {
  name = "${var.project_name}/db-password"
}

resource "aws_secretsmanager_secret_version" "db_password" {
  secret_id     = aws_secretsmanager_secret.db_password.id
  secret_string = jsonencode({
    username = aws_db_instance.postgres.username
    password = random_password.db_password.result
    host     = aws_db_instance.postgres.address
    port     = aws_db_instance.postgres.port
    dbname   = aws_db_instance.postgres.db_name
    url      = "postgresql://${aws_db_instance.postgres.username}:${random_password.db_password.result}@${aws_db_instance.postgres.address}:${aws_db_instance.postgres.port}/${aws_db_instance.postgres.db_name}"
  })
}
```

### Amazon MSK (Kafka) Configuration

Create `deployment/terraform/msk.tf`:

```hcl
resource "aws_msk_cluster" "main" {
  cluster_name           = "${var.project_name}-msk"
  kafka_version          = "3.5.1"
  number_of_broker_nodes = 3

  broker_node_group_info {
    instance_type   = "kafka.t3.small"  # 2 vCPU, 4GB RAM
    client_subnets  = module.vpc.private_subnets
    security_groups = [aws_security_group.msk.id]
    
    storage_info {
      ebs_storage_info {
        volume_size = 100
      }
    }
  }

  encryption_info {
    encryption_in_transit {
      client_broker = "TLS"
      in_cluster    = true
    }
    
    encryption_at_rest_kms_key_arn = aws_kms_key.msk.arn
  }

  configuration_info {
    arn      = aws_msk_configuration.main.arn
    revision = aws_msk_configuration.main.latest_revision
  }

  logging_info {
    broker_logs {
      cloudwatch_logs {
        enabled   = true
        log_group = aws_cloudwatch_log_group.msk.name
      }
    }
  }

  tags = {
    Name = "${var.project_name}-msk"
  }
}

resource "aws_msk_configuration" "main" {
  name              = "${var.project_name}-msk-config"
  kafka_versions    = ["3.5.1"]
  server_properties = <<PROPERTIES
auto.create.topics.enable=true
default.replication.factor=2
min.insync.replicas=1
num.io.threads=8
num.network.threads=5
num.partitions=3
num.replica.fetchers=2
replica.lag.time.max.ms=30000
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
socket.send.buffer.bytes=102400
unclean.leader.election.enable=true
zookeeper.session.timeout.ms=18000
log.retention.hours=168
log.segment.bytes=1073741824
PROPERTIES
}

resource "aws_kms_key" "msk" {
  description = "KMS key for MSK encryption"
}

resource "aws_cloudwatch_log_group" "msk" {
  name              = "/aws/msk/${var.project_name}"
  retention_in_days = 7
}
```

### ElastiCache Redis Configuration

Create `deployment/terraform/elasticache.tf`:

```hcl
resource "aws_elasticache_subnet_group" "main" {
  name       = "${var.project_name}-redis-subnet"
  subnet_ids = module.vpc.private_subnets
}

resource "aws_elasticache_replication_group" "redis" {
  replication_group_id       = "${var.project_name}-redis"
  replication_group_description = "Redis cluster for caching"
  
  engine               = "redis"
  engine_version       = "7.0"
  node_type            = "cache.t3.micro"
  number_cache_clusters = 2
  port                 = 6379
  
  subnet_group_name    = aws_elasticache_subnet_group.main.name
  security_group_ids   = [aws_security_group.elasticache.id]
  
  automatic_failover_enabled = true
  multi_az_enabled          = var.environment == "production" ? true : false
  
  at_rest_encryption_enabled = true
  transit_encryption_enabled = true
  auth_token                 = random_password.redis_password.result
  
  snapshot_retention_limit = 5
  snapshot_window         = "03:00-05:00"
  maintenance_window      = "mon:05:00-mon:07:00"
  
  log_delivery_configuration {
    destination      = aws_cloudwatch_log_group.redis.name
    destination_type = "cloudwatch-logs"
    log_format       = "json"
    log_type         = "slow-log"
  }

  tags = {
    Name = "${var.project_name}-redis"
  }
}

resource "random_password" "redis_password" {
  length  = 32
  special = false
}

resource "aws_secretsmanager_secret" "redis_password" {
  name = "${var.project_name}/redis-password"
}

resource "aws_secretsmanager_secret_version" "redis_password" {
  secret_id     = aws_secretsmanager_secret.redis_password.id
  secret_string = jsonencode({
    password = random_password.redis_password.result
    host     = aws_elasticache_replication_group.redis.primary_endpoint_address
    port     = aws_elasticache_replication_group.redis.port
    url      = "redis://:${random_password.redis_password.result}@${aws_elasticache_replication_group.redis.primary_endpoint_address}:${aws_elasticache_replication_group.redis.port}"
  })
}

resource "aws_cloudwatch_log_group" "redis" {
  name              = "/aws/elasticache/${var.project_name}"
  retention_in_days = 7
}
```

### S3 Storage Configuration

Create `deployment/terraform/s3.tf`:

```hcl
resource "aws_s3_bucket" "models" {
  bucket = "${var.project_name}-models-${data.aws_caller_identity.current.account_id}"

  tags = {
    Name = "${var.project_name}-models"
  }
}

resource "aws_s3_bucket_versioning" "models" {
  bucket = aws_s3_bucket.models.id
  
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_server_side_encryption_configuration" "models" {
  bucket = aws_s3_bucket.models.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

resource "aws_s3_bucket_lifecycle_configuration" "models" {
  bucket = aws_s3_bucket.models.id

  rule {
    id     = "delete-old-versions"
    status = "Enabled"

    noncurrent_version_expiration {
      noncurrent_days = 90
    }
  }
}

resource "aws_s3_bucket" "data" {
  bucket = "${var.project_name}-data-${data.aws_caller_identity.current.account_id}"

  tags = {
    Name = "${var.project_name}-data"
  }
}

resource "aws_s3_bucket_lifecycle_configuration" "data" {
  bucket = aws_s3_bucket.data.id

  rule {
    id     = "archive-old-data"
    status = "Enabled"

    transition {
      days          = 30
      storage_class = "STANDARD_IA"
    }

    transition {
      days          = 90
      storage_class = "GLACIER"
    }
  }
}
```


### ECS Cluster Configuration

Create `deployment/terraform/ecs.tf`:

```hcl
# ECS Cluster
resource "aws_ecs_cluster" "main" {
  name = "${var.project_name}-cluster"

  setting {
    name  = "containerInsights"
    value = "enabled"
  }

  tags = {
    Name = "${var.project_name}-cluster"
  }
}

resource "aws_ecs_cluster_capacity_providers" "main" {
  cluster_name = aws_ecs_cluster.main.name

  capacity_providers = ["FARGATE", "FARGATE_SPOT"]

  default_capacity_provider_strategy {
    capacity_provider = "FARGATE"
    weight            = 1
    base              = 1
  }
}

# ECR Repositories
resource "aws_ecr_repository" "api" {
  name                 = "${var.project_name}/api"
  image_tag_mutability = "MUTABLE"

  image_scanning_configuration {
    scan_on_push = true
  }

  encryption_configuration {
    encryption_type = "AES256"
  }
}

resource "aws_ecr_repository" "dashboard" {
  name                 = "${var.project_name}/dashboard"
  image_tag_mutability = "MUTABLE"

  image_scanning_configuration {
    scan_on_push = true
  }
}

resource "aws_ecr_repository" "workers" {
  name                 = "${var.project_name}/workers"
  image_tag_mutability = "MUTABLE"

  image_scanning_configuration {
    scan_on_push = true
  }
}

# ECR Lifecycle Policies
resource "aws_ecr_lifecycle_policy" "api" {
  repository = aws_ecr_repository.api.name

  policy = jsonencode({
    rules = [{
      rulePriority = 1
      description  = "Keep last 10 images"
      selection = {
        tagStatus     = "any"
        countType     = "imageCountMoreThan"
        countNumber   = 10
      }
      action = {
        type = "expire"
      }
    }]
  })
}
```

### Application Load Balancer

Create `deployment/terraform/alb.tf`:

```hcl
resource "aws_lb" "main" {
  name               = "${var.project_name}-alb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets            = module.vpc.public_subnets

  enable_deletion_protection = var.environment == "production" ? true : false
  enable_http2              = true
  enable_cross_zone_load_balancing = true

  access_logs {
    bucket  = aws_s3_bucket.alb_logs.id
    enabled = true
  }

  tags = {
    Name = "${var.project_name}-alb"
  }
}

resource "aws_s3_bucket" "alb_logs" {
  bucket = "${var.project_name}-alb-logs-${data.aws_caller_identity.current.account_id}"
}

resource "aws_s3_bucket_lifecycle_configuration" "alb_logs" {
  bucket = aws_s3_bucket.alb_logs.id

  rule {
    id     = "delete-old-logs"
    status = "Enabled"

    expiration {
      days = 30
    }
  }
}

# Target Groups
resource "aws_lb_target_group" "api" {
  name        = "${var.project_name}-api-tg"
  port        = 8000
  protocol    = "HTTP"
  vpc_id      = module.vpc.vpc_id
  target_type = "ip"

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher             = "200"
    path                = "/health"
    port                = "traffic-port"
    protocol            = "HTTP"
    timeout             = 5
    unhealthy_threshold = 3
  }

  deregistration_delay = 30
}

resource "aws_lb_target_group" "dashboard" {
  name        = "${var.project_name}-dashboard-tg"
  port        = 8501
  protocol    = "HTTP"
  vpc_id      = module.vpc.vpc_id
  target_type = "ip"

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher             = "200"
    path                = "/"
    port                = "traffic-port"
    protocol            = "HTTP"
    timeout             = 5
    unhealthy_threshold = 3
  }
}

# Listeners
resource "aws_lb_listener" "http" {
  load_balancer_arn = aws_lb.main.arn
  port              = "80"
  protocol          = "HTTP"

  default_action {
    type = "redirect"

    redirect {
      port        = "443"
      protocol    = "HTTPS"
      status_code = "HTTP_301"
    }
  }
}

resource "aws_lb_listener" "https" {
  load_balancer_arn = aws_lb.main.arn
  port              = "443"
  protocol          = "HTTPS"
  ssl_policy        = "ELBSecurityPolicy-TLS-1-2-2017-01"
  certificate_arn   = aws_acm_certificate.main.arn

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.api.arn
  }
}

# Listener Rules
resource "aws_lb_listener_rule" "dashboard" {
  listener_arn = aws_lb_listener.https.arn
  priority     = 100

  action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.dashboard.arn
  }

  condition {
    path_pattern {
      values = ["/dashboard*"]
    }
  }
}
```

### IAM Roles and Policies

Create `deployment/terraform/iam.tf`:

```hcl
# ECS Task Execution Role
resource "aws_iam_role" "ecs_task_execution" {
  name = "${var.project_name}-ecs-task-execution"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "ecs-tasks.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "ecs_task_execution" {
  role       = aws_iam_role.ecs_task_execution.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy"
}

resource "aws_iam_role_policy" "ecs_task_execution_secrets" {
  name = "secrets-access"
  role = aws_iam_role.ecs_task_execution.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Action = [
        "secretsmanager:GetSecretValue",
        "kms:Decrypt"
      ]
      Resource = [
        aws_secretsmanager_secret.db_password.arn,
        aws_secretsmanager_secret.redis_password.arn
      ]
    }]
  })
}

# ECS Task Role
resource "aws_iam_role" "ecs_task" {
  name = "${var.project_name}-ecs-task"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "ecs-tasks.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_role_policy" "ecs_task_s3" {
  name = "s3-access"
  role = aws_iam_role.ecs_task.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Action = [
        "s3:GetObject",
        "s3:PutObject",
        "s3:ListBucket"
      ]
      Resource = [
        aws_s3_bucket.models.arn,
        "${aws_s3_bucket.models.arn}/*",
        aws_s3_bucket.data.arn,
        "${aws_s3_bucket.data.arn}/*"
      ]
    }]
  })
}

resource "aws_iam_role_policy" "ecs_task_cloudwatch" {
  name = "cloudwatch-access"
  role = aws_iam_role.ecs_task.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Action = [
        "logs:CreateLogGroup",
        "logs:CreateLogStream",
        "logs:PutLogEvents",
        "cloudwatch:PutMetricData"
      ]
      Resource = "*"
    }]
  })
}
```

### ECS Task Definitions

Create `deployment/terraform/ecs-tasks.tf`:

```hcl
# API Service Task Definition
resource "aws_ecs_task_definition" "api" {
  family                   = "${var.project_name}-api"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "2048"  # 2 vCPU
  memory                   = "4096"  # 4 GB
  execution_role_arn       = aws_iam_role.ecs_task_execution.arn
  task_role_arn            = aws_iam_role.ecs_task.arn

  container_definitions = jsonencode([{
    name  = "api"
    image = "${aws_ecr_repository.api.repository_url}:latest"
    
    portMappings = [{
      containerPort = 8000
      protocol      = "tcp"
    }]

    environment = [
      {
        name  = "ENVIRONMENT"
        value = var.environment
      },
      {
        name  = "AWS_REGION"
        value = var.aws_region
      },
      {
        name  = "MSK_BOOTSTRAP_SERVERS"
        value = aws_msk_cluster.main.bootstrap_brokers_tls
      },
      {
        name  = "S3_MODELS_BUCKET"
        value = aws_s3_bucket.models.id
      }
    ]

    secrets = [
      {
        name      = "DATABASE_URL"
        valueFrom = "${aws_secretsmanager_secret.db_password.arn}:url::"
      },
      {
        name      = "REDIS_URL"
        valueFrom = "${aws_secretsmanager_secret.redis_password.arn}:url::"
      }
    ]

    logConfiguration = {
      logDriver = "awslogs"
      options = {
        "awslogs-group"         = aws_cloudwatch_log_group.api.name
        "awslogs-region"        = var.aws_region
        "awslogs-stream-prefix" = "api"
      }
    }

    healthCheck = {
      command     = ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval    = 30
      timeout     = 5
      retries     = 3
      startPeriod = 60
    }
  }])
}

# Dashboard Service Task Definition
resource "aws_ecs_task_definition" "dashboard" {
  family                   = "${var.project_name}-dashboard"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "1024"  # 1 vCPU
  memory                   = "2048"  # 2 GB
  execution_role_arn       = aws_iam_role.ecs_task_execution.arn
  task_role_arn            = aws_iam_role.ecs_task.arn

  container_definitions = jsonencode([{
    name  = "dashboard"
    image = "${aws_ecr_repository.dashboard.repository_url}:latest"
    
    portMappings = [{
      containerPort = 8501
      protocol      = "tcp"
    }]

    environment = [
      {
        name  = "API_URL"
        value = "http://${aws_lb.main.dns_name}"
      }
    ]

    secrets = [
      {
        name      = "DATABASE_URL"
        valueFrom = "${aws_secretsmanager_secret.db_password.arn}:url::"
      }
    ]

    logConfiguration = {
      logDriver = "awslogs"
      options = {
        "awslogs-group"         = aws_cloudwatch_log_group.dashboard.name
        "awslogs-region"        = var.aws_region
        "awslogs-stream-prefix" = "dashboard"
      }
    }
  }])
}

# Feature Processor Worker Task Definition
resource "aws_ecs_task_definition" "feature_processor" {
  family                   = "${var.project_name}-feature-processor"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "2048"
  memory                   = "4096"
  execution_role_arn       = aws_iam_role.ecs_task_execution.arn
  task_role_arn            = aws_iam_role.ecs_task.arn

  container_definitions = jsonencode([{
    name    = "feature-processor"
    image   = "${aws_ecr_repository.workers.repository_url}:latest"
    command = ["python", "-m", "src.streaming.feature_processor"]

    environment = [
      {
        name  = "MSK_BOOTSTRAP_SERVERS"
        value = aws_msk_cluster.main.bootstrap_brokers_tls
      },
      {
        name  = "API_URL"
        value = "http://${aws_lb.main.dns_name}"
      }
    ]

    secrets = [
      {
        name      = "DATABASE_URL"
        valueFrom = "${aws_secretsmanager_secret.db_password.arn}:url::"
      }
    ]

    logConfiguration = {
      logDriver = "awslogs"
      options = {
        "awslogs-group"         = aws_cloudwatch_log_group.workers.name
        "awslogs-region"        = var.aws_region
        "awslogs-stream-prefix" = "feature-processor"
      }
    }
  }])
}

# Intervention Worker Task Definition
resource "aws_ecs_task_definition" "intervention_worker" {
  family                   = "${var.project_name}-intervention-worker"
  network_mode             = "awsvpc"
  requires_compatibilities = ["FARGATE"]
  cpu                      = "1024"
  memory                   = "2048"
  execution_role_arn       = aws_iam_role.ecs_task_execution.arn
  task_role_arn            = aws_iam_role.ecs_task.arn

  container_definitions = jsonencode([{
    name    = "intervention-worker"
    image   = "${aws_ecr_repository.workers.repository_url}:latest"
    command = ["python", "-m", "src.streaming.intervention_worker"]

    environment = [
      {
        name  = "MSK_BOOTSTRAP_SERVERS"
        value = aws_msk_cluster.main.bootstrap_brokers_tls
      }
    ]

    secrets = [
      {
        name      = "DATABASE_URL"
        valueFrom = "${aws_secretsmanager_secret.db_password.arn}:url::"
      }
    ]

    logConfiguration = {
      logDriver = "awslogs"
      options = {
        "awslogs-group"         = aws_cloudwatch_log_group.workers.name
        "awslogs-region"        = var.aws_region
        "awslogs-stream-prefix" = "intervention-worker"
      }
    }
  }])
}
```

### ECS Services with Auto Scaling

Create `deployment/terraform/ecs-services.tf`:

```hcl
# API Service
resource "aws_ecs_service" "api" {
  name            = "${var.project_name}-api"
  cluster         = aws_ecs_cluster.main.id
  task_definition = aws_ecs_task_definition.api.arn
  desired_count   = 2
  launch_type     = "FARGATE"

  network_configuration {
    subnets          = module.vpc.private_subnets
    security_groups  = [aws_security_group.ecs_tasks.id]
    assign_public_ip = false
  }

  load_balancer {
    target_group_arn = aws_lb_target_group.api.arn
    container_name   = "api"
    container_port   = 8000
  }

  deployment_configuration {
    maximum_percent         = 200
    minimum_healthy_percent = 100
  }

  depends_on = [aws_lb_listener.https]
}

# API Auto Scaling
resource "aws_appautoscaling_target" "api" {
  max_capacity       = 10
  min_capacity       = 2
  resource_id        = "service/${aws_ecs_cluster.main.name}/${aws_ecs_service.api.name}"
  scalable_dimension = "ecs:service:DesiredCount"
  service_namespace  = "ecs"
}

resource "aws_appautoscaling_policy" "api_cpu" {
  name               = "${var.project_name}-api-cpu-scaling"
  policy_type        = "TargetTrackingScaling"
  resource_id        = aws_appautoscaling_target.api.resource_id
  scalable_dimension = aws_appautoscaling_target.api.scalable_dimension
  service_namespace  = aws_appautoscaling_target.api.service_namespace

  target_tracking_scaling_policy_configuration {
    predefined_metric_specification {
      predefined_metric_type = "ECSServiceAverageCPUUtilization"
    }
    target_value = 70.0
  }
}

resource "aws_appautoscaling_policy" "api_memory" {
  name               = "${var.project_name}-api-memory-scaling"
  policy_type        = "TargetTrackingScaling"
  resource_id        = aws_appautoscaling_target.api.resource_id
  scalable_dimension = aws_appautoscaling_target.api.scalable_dimension
  service_namespace  = aws_appautoscaling_target.api.service_namespace

  target_tracking_scaling_policy_configuration {
    predefined_metric_specification {
      predefined_metric_type = "ECSServiceAverageMemoryUtilization"
    }
    target_value = 80.0
  }
}

# Dashboard Service
resource "aws_ecs_service" "dashboard" {
  name            = "${var.project_name}-dashboard"
  cluster         = aws_ecs_cluster.main.id
  task_definition = aws_ecs_task_definition.dashboard.arn
  desired_count   = 1
  launch_type     = "FARGATE"

  network_configuration {
    subnets          = module.vpc.private_subnets
    security_groups  = [aws_security_group.ecs_tasks.id]
    assign_public_ip = false
  }

  load_balancer {
    target_group_arn = aws_lb_target_group.dashboard.arn
    container_name   = "dashboard"
    container_port   = 8501
  }

  depends_on = [aws_lb_listener.https]
}

# Feature Processor Service
resource "aws_ecs_service" "feature_processor" {
  name            = "${var.project_name}-feature-processor"
  cluster         = aws_ecs_cluster.main.id
  task_definition = aws_ecs_task_definition.feature_processor.arn
  desired_count   = 2
  launch_type     = "FARGATE"

  network_configuration {
    subnets          = module.vpc.private_subnets
    security_groups  = [aws_security_group.ecs_tasks.id]
    assign_public_ip = false
  }
}

resource "aws_appautoscaling_target" "feature_processor" {
  max_capacity       = 5
  min_capacity       = 2
  resource_id        = "service/${aws_ecs_cluster.main.name}/${aws_ecs_service.feature_processor.name}"
  scalable_dimension = "ecs:service:DesiredCount"
  service_namespace  = "ecs"
}

# Intervention Worker Service
resource "aws_ecs_service" "intervention_worker" {
  name            = "${var.project_name}-intervention-worker"
  cluster         = aws_ecs_cluster.main.id
  task_definition = aws_ecs_task_definition.intervention_worker.arn
  desired_count   = 1
  launch_type     = "FARGATE"

  network_configuration {
    subnets          = module.vpc.private_subnets
    security_groups  = [aws_security_group.ecs_tasks.id]
    assign_public_ip = false
  }
}
```

### CloudWatch Monitoring

Create `deployment/terraform/cloudwatch.tf`:

```hcl
# Log Groups
resource "aws_cloudwatch_log_group" "api" {
  name              = "/ecs/${var.project_name}/api"
  retention_in_days = 7
}

resource "aws_cloudwatch_log_group" "dashboard" {
  name              = "/ecs/${var.project_name}/dashboard"
  retention_in_days = 7
}

resource "aws_cloudwatch_log_group" "workers" {
  name              = "/ecs/${var.project_name}/workers"
  retention_in_days = 7
}

# CloudWatch Dashboard
resource "aws_cloudwatch_dashboard" "main" {
  dashboard_name = "${var.project_name}-dashboard"

  dashboard_body = jsonencode({
    widgets = [
      {
        type = "metric"
        properties = {
          metrics = [
            ["AWS/ECS", "CPUUtilization", { stat = "Average" }],
            [".", "MemoryUtilization", { stat = "Average" }]
          ]
          period = 300
          stat   = "Average"
          region = var.aws_region
          title  = "ECS Cluster Metrics"
        }
      },
      {
        type = "metric"
        properties = {
          metrics = [
            ["AWS/ApplicationELB", "TargetResponseTime", { stat = "Average" }],
            [".", "RequestCount", { stat = "Sum" }]
          ]
          period = 300
          stat   = "Average"
          region = var.aws_region
          title  = "ALB Metrics"
        }
      },
      {
        type = "metric"
        properties = {
          metrics = [
            ["AWS/RDS", "CPUUtilization", { stat = "Average" }],
            [".", "DatabaseConnections", { stat = "Average" }],
            [".", "ReadLatency", { stat = "Average" }],
            [".", "WriteLatency", { stat = "Average" }]
          ]
          period = 300
          stat   = "Average"
          region = var.aws_region
          title  = "RDS Metrics"
        }
      },
      {
        type = "metric"
        properties = {
          metrics = [
            ["AWS/Kafka", "BytesInPerSec", { stat = "Sum" }],
            [".", "BytesOutPerSec", { stat = "Sum" }]
          ]
          period = 300
          stat   = "Sum"
          region = var.aws_region
          title  = "MSK Metrics"
        }
      }
    ]
  })
}

# CloudWatch Alarms
resource "aws_cloudwatch_metric_alarm" "api_cpu_high" {
  alarm_name          = "${var.project_name}-api-cpu-high"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "CPUUtilization"
  namespace           = "AWS/ECS"
  period              = "300"
  statistic           = "Average"
  threshold           = "80"
  alarm_description   = "API CPU utilization is too high"
  alarm_actions       = [aws_sns_topic.alerts.arn]

  dimensions = {
    ClusterName = aws_ecs_cluster.main.name
    ServiceName = aws_ecs_service.api.name
  }
}

resource "aws_cloudwatch_metric_alarm" "api_memory_high" {
  alarm_name          = "${var.project_name}-api-memory-high"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "MemoryUtilization"
  namespace           = "AWS/ECS"
  period              = "300"
  statistic           = "Average"
  threshold           = "85"
  alarm_description   = "API memory utilization is too high"
  alarm_actions       = [aws_sns_topic.alerts.arn]

  dimensions = {
    ClusterName = aws_ecs_cluster.main.name
    ServiceName = aws_ecs_service.api.name
  }
}

resource "aws_cloudwatch_metric_alarm" "rds_cpu_high" {
  alarm_name          = "${var.project_name}-rds-cpu-high"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "CPUUtilization"
  namespace           = "AWS/RDS"
  period              = "300"
  statistic           = "Average"
  threshold           = "80"
  alarm_description   = "RDS CPU utilization is too high"
  alarm_actions       = [aws_sns_topic.alerts.arn]

  dimensions = {
    DBInstanceIdentifier = aws_db_instance.postgres.id
  }
}

resource "aws_cloudwatch_metric_alarm" "alb_5xx_errors" {
  alarm_name          = "${var.project_name}-alb-5xx-errors"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "1"
  metric_name         = "HTTPCode_Target_5XX_Count"
  namespace           = "AWS/ApplicationELB"
  period              = "300"
  statistic           = "Sum"
  threshold           = "10"
  alarm_description   = "Too many 5XX errors from ALB"
  alarm_actions       = [aws_sns_topic.alerts.arn]

  dimensions = {
    LoadBalancer = aws_lb.main.arn_suffix
  }
}

# SNS Topic for Alerts
resource "aws_sns_topic" "alerts" {
  name = "${var.project_name}-alerts"
}

resource "aws_sns_topic_subscription" "alerts_email" {
  topic_arn = aws_sns_topic.alerts.arn
  protocol  = "email"
  endpoint  = var.alert_email
}
```

### Variables and Outputs

Create `deployment/terraform/variables.tf`:

```hcl
variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "us-east-1"
}

variable "project_name" {
  description = "Project name"
  type        = string
  default     = "pre-delinquency-engine"
}

variable "environment" {
  description = "Environment (dev, staging, production)"
  type        = string
  default     = "production"
}

variable "db_instance_class" {
  description = "RDS instance class"
  type        = string
  default     = "db.t3.medium"
}

variable "alert_email" {
  description = "Email for CloudWatch alerts"
  type        = string
}

variable "domain_name" {
  description = "Domain name for the application"
  type        = string
  default     = ""
}
```

Create `deployment/terraform/outputs.tf`:

```hcl
output "vpc_id" {
  description = "VPC ID"
  value       = module.vpc.vpc_id
}

output "alb_dns_name" {
  description = "ALB DNS name"
  value       = aws_lb.main.dns_name
}

output "api_url" {
  description = "API URL"
  value       = "https://${aws_lb.main.dns_name}"
}

output "dashboard_url" {
  description = "Dashboard URL"
  value       = "https://${aws_lb.main.dns_name}/dashboard"
}

output "rds_endpoint" {
  description = "RDS endpoint"
  value       = aws_db_instance.postgres.endpoint
  sensitive   = true
}

output "redis_endpoint" {
  description = "Redis endpoint"
  value       = aws_elasticache_replication_group.redis.primary_endpoint_address
  sensitive   = true
}

output "msk_bootstrap_servers" {
  description = "MSK bootstrap servers"
  value       = aws_msk_cluster.main.bootstrap_brokers_tls
  sensitive   = true
}

output "ecr_api_repository_url" {
  description = "ECR API repository URL"
  value       = aws_ecr_repository.api.repository_url
}

output "ecr_dashboard_repository_url" {
  description = "ECR dashboard repository URL"
  value       = aws_ecr_repository.dashboard.repository_url
}

output "ecr_workers_repository_url" {
  description = "ECR workers repository URL"
  value       = aws_ecr_repository.workers.repository_url
}

output "s3_models_bucket" {
  description = "S3 models bucket"
  value       = aws_s3_bucket.models.id
}

output "s3_data_bucket" {
  description = "S3 data bucket"
  value       = aws_s3_bucket.data.id
}

output "cloudwatch_dashboard_url" {
  description = "CloudWatch dashboard URL"
  value       = "https://console.aws.amazon.com/cloudwatch/home?region=${var.aws_region}#dashboards:name=${aws_cloudwatch_dashboard.main.dashboard_name}"
}
```

---

## Phase 2: Build and Push Container Images

Create `deployment/scripts/build-images.sh`:

```bash
#!/bin/bash
set -e

# Load environment variables
source .env.aws

# Get AWS account ID
AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

# ECR login
aws ecr get-login-password --region $AWS_REGION | \
  docker login --username AWS --password-stdin \
  $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com

echo "Building and pushing Docker images..."

# Build and push API image
echo "Building API image..."
docker build -t $PROJECT_NAME/api:latest -f docker/Dockerfile.api .
docker tag $PROJECT_NAME/api:latest \
  $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$PROJECT_NAME/api:latest
docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$PROJECT_NAME/api:latest

# Build and push Dashboard image
echo "Building Dashboard image..."
docker build -t $PROJECT_NAME/dashboard:latest -f docker/Dockerfile.dashboard .
docker tag $PROJECT_NAME/dashboard:latest \
  $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$PROJECT_NAME/dashboard:latest
docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$PROJECT_NAME/dashboard:latest

# Build and push Workers image
echo "Building Workers image..."
docker build -t $PROJECT_NAME/workers:latest -f docker/Dockerfile.worker .
docker tag $PROJECT_NAME/workers:latest \
  $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$PROJECT_NAME/workers:latest
docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/$PROJECT_NAME/workers:latest

echo "‚úÖ All images built and pushed successfully!"
```

Make it executable:
```bash
chmod +x deployment/scripts/build-images.sh
```

---

## Phase 3: Database Migration

Create `deployment/scripts/migrate-db.sh`:

```bash
#!/bin/bash
set -e

echo "Running database migrations..."

# Get RDS endpoint from Terraform output
RDS_ENDPOINT=$(terraform output -raw rds_endpoint)
DB_PASSWORD=$(aws secretsmanager get-secret-value \
  --secret-id pre-delinquency-engine/db-password \
  --query SecretString --output text | jq -r .password)

# Export connection string
export DATABASE_URL="postgresql://admin:$DB_PASSWORD@$RDS_ENDPOINT/bank_data"

# Run migrations
python -m src.data_generation.check_db

# Upload model to S3
S3_MODELS_BUCKET=$(terraform output -raw s3_models_bucket)
aws s3 cp data/models/quick/quick_model.json \
  s3://$S3_MODELS_BUCKET/models/quick_model.json

echo "‚úÖ Database migration complete!"
```

Make it executable:
```bash
chmod +x deployment/scripts/migrate-db.sh
```

---

## Phase 4: Complete Deployment Script

Create `deployment/scripts/deploy.sh`:

```bash
#!/bin/bash
# Pre-Delinquency Engine - AWS Deployment Script
# Usage: ./deploy.sh [dev|staging|production]

set -e

ENVIRONMENT=${1:-production}
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"

echo "üöÄ Deploying Pre-Delinquency Engine to AWS ($ENVIRONMENT)"
echo "============================================================"

# Load environment variables
if [ -f "$PROJECT_ROOT/.env.aws" ]; then
  source "$PROJECT_ROOT/.env.aws"
else
  echo "‚ùå Error: .env.aws file not found"
  exit 1
fi

# Step 1: Validate AWS credentials
echo ""
echo "Step 1: Validating AWS credentials..."
aws sts get-caller-identity > /dev/null
if [ $? -eq 0 ]; then
  echo "‚úÖ AWS credentials valid"
else
  echo "‚ùå AWS credentials invalid. Run 'aws configure'"
  exit 1
fi

# Step 2: Initialize Terraform
echo ""
echo "Step 2: Initializing Terraform..."
cd "$PROJECT_ROOT/deployment/terraform"
terraform init

# Step 3: Plan infrastructure
echo ""
echo "Step 3: Planning infrastructure..."
terraform plan \
  -var="environment=$ENVIRONMENT" \
  -var="alert_email=${ALERT_EMAIL:-admin@example.com}" \
  -out=tfplan

# Step 4: Apply infrastructure
echo ""
echo "Step 4: Applying infrastructure..."
read -p "Apply Terraform plan? (yes/no): " confirm
if [ "$confirm" != "yes" ]; then
  echo "Deployment cancelled"
  exit 0
fi

terraform apply tfplan

# Step 5: Build and push Docker images
echo ""
echo "Step 5: Building and pushing Docker images..."
cd "$PROJECT_ROOT"
bash deployment/scripts/build-images.sh

# Step 6: Run database migrations
echo ""
echo "Step 6: Running database migrations..."
bash deployment/scripts/migrate-db.sh

# Step 7: Update ECS services
echo ""
echo "Step 7: Updating ECS services..."
aws ecs update-service \
  --cluster $PROJECT_NAME-cluster \
  --service $PROJECT_NAME-api \
  --force-new-deployment \
  --region $AWS_REGION

aws ecs update-service \
  --cluster $PROJECT_NAME-cluster \
  --service $PROJECT_NAME-dashboard \
  --force-new-deployment \
  --region $AWS_REGION

aws ecs update-service \
  --cluster $PROJECT_NAME-cluster \
  --service $PROJECT_NAME-feature-processor \
  --force-new-deployment \
  --region $AWS_REGION

aws ecs update-service \
  --cluster $PROJECT_NAME-cluster \
  --service $PROJECT_NAME-intervention-worker \
  --force-new-deployment \
  --region $AWS_REGION

# Step 8: Wait for services to stabilize
echo ""
echo "Step 8: Waiting for services to stabilize..."
aws ecs wait services-stable \
  --cluster $PROJECT_NAME-cluster \
  --services $PROJECT_NAME-api \
  --region $AWS_REGION

# Step 9: Get deployment URLs
echo ""
echo "Step 9: Getting deployment URLs..."
cd "$PROJECT_ROOT/deployment/terraform"
API_URL=$(terraform output -raw api_url)
DASHBOARD_URL=$(terraform output -raw dashboard_url)
CLOUDWATCH_URL=$(terraform output -raw cloudwatch_dashboard_url)

# Step 10: Test deployment
echo ""
echo "Step 10: Testing deployment..."
HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" "$API_URL/health")
if [ "$HTTP_CODE" == "200" ]; then
  echo "‚úÖ API health check passed"
else
  echo "‚ö†Ô∏è  API health check returned $HTTP_CODE"
fi

echo ""
echo "============================================================"
echo "‚úÖ Deployment Complete!"
echo "============================================================"
echo ""
echo "üìç API URL:        $API_URL"
echo "üìç Dashboard URL:  $DASHBOARD_URL"
echo "üìç CloudWatch:     $CLOUDWATCH_URL"
echo ""
echo "Next steps:"
echo "1. Test API: curl $API_URL/health"
echo "2. View dashboard: open $DASHBOARD_URL"
echo "3. Monitor logs: aws logs tail /ecs/$PROJECT_NAME/api --follow"
echo "4. View metrics: open $CLOUDWATCH_URL"
echo ""
```

Make it executable:
```bash
chmod +x deployment/scripts/deploy.sh
```

---

## Phase 5: CI/CD Pipeline (AWS CodePipeline)

Create `deployment/codepipeline/buildspec.yml`:

```yaml
version: 0.2

phases:
  pre_build:
    commands:
      - echo Logging in to Amazon ECR...
      - aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com
      - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)
      - IMAGE_TAG=${COMMIT_HASH:=latest}
  
  build:
    commands:
      - echo Build started on `date`
      - echo Building API Docker image...
      - docker build -t $ECR_API_REPO:latest -f docker/Dockerfile.api .
      - docker tag $ECR_API_REPO:latest $ECR_API_REPO:$IMAGE_TAG
      
      - echo Building Dashboard Docker image...
      - docker build -t $ECR_DASHBOARD_REPO:latest -f docker/Dockerfile.dashboard .
      - docker tag $ECR_DASHBOARD_REPO:latest $ECR_DASHBOARD_REPO:$IMAGE_TAG
      
      - echo Building Workers Docker image...
      - docker build -t $ECR_WORKERS_REPO:latest -f docker/Dockerfile.worker .
      - docker tag $ECR_WORKERS_REPO:latest $ECR_WORKERS_REPO:$IMAGE_TAG
  
  post_build:
    commands:
      - echo Build completed on `date`
      - echo Pushing Docker images...
      - docker push $ECR_API_REPO:latest
      - docker push $ECR_API_REPO:$IMAGE_TAG
      - docker push $ECR_DASHBOARD_REPO:latest
      - docker push $ECR_DASHBOARD_REPO:$IMAGE_TAG
      - docker push $ECR_WORKERS_REPO:latest
      - docker push $ECR_WORKERS_REPO:$IMAGE_TAG
      
      - echo Writing image definitions file...
      - printf '[{"name":"api","imageUri":"%s"},{"name":"dashboard","imageUri":"%s"},{"name":"workers","imageUri":"%s"}]' $ECR_API_REPO:$IMAGE_TAG $ECR_DASHBOARD_REPO:$IMAGE_TAG $ECR_WORKERS_REPO:$IMAGE_TAG > imagedefinitions.json

artifacts:
  files:
    - imagedefinitions.json
```

Create CodePipeline Terraform configuration `deployment/terraform/codepipeline.tf`:

```hcl
# CodeBuild Project
resource "aws_codebuild_project" "main" {
  name          = "${var.project_name}-build"
  service_role  = aws_iam_role.codebuild.arn
  build_timeout = 30

  artifacts {
    type = "CODEPIPELINE"
  }

  environment {
    compute_type                = "BUILD_GENERAL1_MEDIUM"
    image                       = "aws/codebuild/standard:7.0"
    type                        = "LINUX_CONTAINER"
    privileged_mode             = true
    image_pull_credentials_type = "CODEBUILD"

    environment_variable {
      name  = "AWS_REGION"
      value = var.aws_region
    }

    environment_variable {
      name  = "AWS_ACCOUNT_ID"
      value = data.aws_caller_identity.current.account_id
    }

    environment_variable {
      name  = "ECR_API_REPO"
      value = aws_ecr_repository.api.repository_url
    }

    environment_variable {
      name  = "ECR_DASHBOARD_REPO"
      value = aws_ecr_repository.dashboard.repository_url
    }

    environment_variable {
      name  = "ECR_WORKERS_REPO"
      value = aws_ecr_repository.workers.repository_url
    }
  }

  source {
    type      = "CODEPIPELINE"
    buildspec = "deployment/codepipeline/buildspec.yml"
  }
}

# CodePipeline
resource "aws_codepipeline" "main" {
  name     = "${var.project_name}-pipeline"
  role_arn = aws_iam_role.codepipeline.arn

  artifact_store {
    location = aws_s3_bucket.pipeline_artifacts.bucket
    type     = "S3"
  }

  stage {
    name = "Source"

    action {
      name             = "Source"
      category         = "Source"
      owner            = "AWS"
      provider         = "CodeStarSourceConnection"
      version          = "1"
      output_artifacts = ["source_output"]

      configuration = {
        ConnectionArn    = var.github_connection_arn
        FullRepositoryId = var.github_repo
        BranchName       = var.github_branch
      }
    }
  }

  stage {
    name = "Build"

    action {
      name             = "Build"
      category         = "Build"
      owner            = "AWS"
      provider         = "CodeBuild"
      input_artifacts  = ["source_output"]
      output_artifacts = ["build_output"]
      version          = "1"

      configuration = {
        ProjectName = aws_codebuild_project.main.name
      }
    }
  }

  stage {
    name = "Deploy"

    action {
      name            = "Deploy-API"
      category        = "Deploy"
      owner           = "AWS"
      provider        = "ECS"
      input_artifacts = ["build_output"]
      version         = "1"

      configuration = {
        ClusterName = aws_ecs_cluster.main.name
        ServiceName = aws_ecs_service.api.name
        FileName    = "imagedefinitions.json"
      }
    }
  }
}

# S3 Bucket for Pipeline Artifacts
resource "aws_s3_bucket" "pipeline_artifacts" {
  bucket = "${var.project_name}-pipeline-artifacts-${data.aws_caller_identity.current.account_id}"
}

resource "aws_s3_bucket_versioning" "pipeline_artifacts" {
  bucket = aws_s3_bucket.pipeline_artifacts.id
  
  versioning_configuration {
    status = "Enabled"
  }
}

# IAM Roles
resource "aws_iam_role" "codebuild" {
  name = "${var.project_name}-codebuild-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "codebuild.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_role_policy" "codebuild" {
  role = aws_iam_role.codebuild.name

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents"
        ]
        Resource = "*"
      },
      {
        Effect = "Allow"
        Action = [
          "ecr:GetAuthorizationToken",
          "ecr:BatchCheckLayerAvailability",
          "ecr:GetDownloadUrlForLayer",
          "ecr:BatchGetImage",
          "ecr:PutImage",
          "ecr:InitiateLayerUpload",
          "ecr:UploadLayerPart",
          "ecr:CompleteLayerUpload"
        ]
        Resource = "*"
      },
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:PutObject"
        ]
        Resource = "${aws_s3_bucket.pipeline_artifacts.arn}/*"
      }
    ]
  })
}

resource "aws_iam_role" "codepipeline" {
  name = "${var.project_name}-codepipeline-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "codepipeline.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_role_policy" "codepipeline" {
  role = aws_iam_role.codepipeline.name

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:PutObject"
        ]
        Resource = "${aws_s3_bucket.pipeline_artifacts.arn}/*"
      },
      {
        Effect = "Allow"
        Action = [
          "codebuild:BatchGetBuilds",
          "codebuild:StartBuild"
        ]
        Resource = aws_codebuild_project.main.arn
      },
      {
        Effect = "Allow"
        Action = [
          "ecs:UpdateService",
          "ecs:DescribeServices",
          "ecs:DescribeTaskDefinition",
          "ecs:RegisterTaskDefinition"
        ]
        Resource = "*"
      },
      {
        Effect = "Allow"
        Action = [
          "iam:PassRole"
        ]
        Resource = [
          aws_iam_role.ecs_task_execution.arn,
          aws_iam_role.ecs_task.arn
        ]
      },
      {
        Effect = "Allow"
        Action = [
          "codestar-connections:UseConnection"
        ]
        Resource = var.github_connection_arn
      }
    ]
  })
}
```

---

## Quick Start Guide

### 1. Prerequisites Setup

```bash
# Install required tools
pip install awscli terraform

# Configure AWS credentials
aws configure

# Clone repository
git clone https://github.com/your-repo/pre-delinquency-engine.git
cd pre-delinquency-engine

# Create environment file
cat > .env.aws << EOF
AWS_REGION=us-east-1
AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
PROJECT_NAME=pre-delinquency-engine
ENVIRONMENT=production
ALERT_EMAIL=your-email@example.com
EOF
```

### 2. Deploy Infrastructure

```bash
# Initialize Terraform
cd deployment/terraform
terraform init

# Review plan
terraform plan -var="alert_email=your-email@example.com"

# Apply infrastructure
terraform apply -var="alert_email=your-email@example.com"
```

### 3. Build and Deploy Application

```bash
# Build and push images
cd ../..
bash deployment/scripts/build-images.sh

# Run database migrations
bash deployment/scripts/migrate-db.sh

# Deploy services
bash deployment/scripts/deploy.sh production
```

### 4. Verify Deployment

```bash
# Get API URL
API_URL=$(cd deployment/terraform && terraform output -raw api_url)

# Test API
curl $API_URL/health

# View logs
aws logs tail /ecs/pre-delinquency-engine/api --follow
```

---

## Cost Estimation

### Monthly AWS Costs (Production)

| Service | Configuration | Monthly Cost |
|---------|--------------|--------------|
| **ECS Fargate** | API (2-10 tasks, 2 vCPU, 4GB) | $60-300 |
| **ECS Fargate** | Dashboard (1-5 tasks, 1 vCPU, 2GB) | $15-75 |
| **ECS Fargate** | Workers (3 tasks, 1-2 vCPU, 2-4GB) | $45-135 |
| **Amazon MSK** | 3 brokers, kafka.t3.small | $150 |
| **RDS PostgreSQL** | db.t3.medium, Multi-AZ, 100GB | $120 |
| **ElastiCache Redis** | cache.t3.micro, Multi-AZ | $40 |
| **Application Load Balancer** | 1 ALB | $25 |
| **NAT Gateway** | 3 AZs | $100 |
| **S3** | 50GB storage, 1M requests | $5 |
| **CloudWatch** | Logs, metrics, alarms | $20 |
| **Data Transfer** | 100GB/month | $10 |
| **Total** | | **$590-880/month** |

### Cost Optimization Tips

1. **Use Fargate Spot** for non-critical workers (70% savings)
2. **Single NAT Gateway** for dev/staging ($67 savings)
3. **Reserved Instances** for RDS (40% savings)
4. **S3 Lifecycle Policies** to archive old data
5. **CloudWatch Log Retention** set to 7 days
6. **Auto-scaling** to scale down during off-hours

### Development Environment Costs

| Service | Configuration | Monthly Cost |
|---------|--------------|--------------|
| **ECS Fargate** | Minimal tasks | $30 |
| **Amazon MSK** | 2 brokers, kafka.t3.small | $100 |
| **RDS PostgreSQL** | db.t3.micro, Single-AZ | $20 |
| **ElastiCache Redis** | cache.t3.micro, Single-AZ | $15 |
| **Other Services** | ALB, S3, CloudWatch | $30 |
| **Total** | | **$195/month** |

---

## Monitoring and Observability

### CloudWatch Dashboards

Access your dashboard:
```bash
DASHBOARD_URL=$(cd deployment/terraform && terraform output -raw cloudwatch_dashboard_url)
open $DASHBOARD_URL
```

### Key Metrics to Monitor

1. **ECS Metrics**
   - CPU Utilization (target: <70%)
   - Memory Utilization (target: <80%)
   - Task Count
   - Service Health

2. **ALB Metrics**
   - Request Count
   - Target Response Time (target: <500ms)
   - HTTP 4XX/5XX Errors
   - Healthy/Unhealthy Targets

3. **RDS Metrics**
   - CPU Utilization (target: <70%)
   - Database Connections
   - Read/Write Latency
   - Storage Space

4. **MSK Metrics**
   - Bytes In/Out Per Second
   - Consumer Lag
   - Partition Count
   - Broker CPU

5. **Application Metrics**
   - Predictions Per Second
   - Interventions Triggered
   - API Response Time
   - Error Rate

### Log Analysis

```bash
# View API logs
aws logs tail /ecs/pre-delinquency-engine/api --follow

# View worker logs
aws logs tail /ecs/pre-delinquency-engine/workers --follow

# Search for errors
aws logs filter-pattern /ecs/pre-delinquency-engine/api --filter-pattern "ERROR"

# View specific time range
aws logs tail /ecs/pre-delinquency-engine/api \
  --since 1h \
  --format short
```

### X-Ray Distributed Tracing

Enable X-Ray in your application:

```python
# Add to src/serving/api.py
from aws_xray_sdk.core import xray_recorder
from aws_xray_sdk.ext.flask.middleware import XRayMiddleware

xray_recorder.configure(service='pre-delinquency-api')
XRayMiddleware(app, xray_recorder)
```

View traces:
```bash
# Open X-Ray console
open "https://console.aws.amazon.com/xray/home?region=us-east-1#/service-map"
```

---

## Security Best Practices

### 1. Network Security

- ‚úÖ Private subnets for all compute resources
- ‚úÖ Security groups with least privilege
- ‚úÖ VPC Flow Logs enabled
- ‚úÖ TLS encryption for all traffic

### 2. Data Encryption

- ‚úÖ RDS encryption at rest (KMS)
- ‚úÖ ElastiCache encryption in transit and at rest
- ‚úÖ MSK encryption in transit and at rest
- ‚úÖ S3 encryption (AES-256)

### 3. Secrets Management

- ‚úÖ AWS Secrets Manager for credentials
- ‚úÖ No hardcoded secrets in code
- ‚úÖ Automatic secret rotation
- ‚úÖ IAM roles for service authentication

### 4. Access Control

- ‚úÖ IAM roles with least privilege
- ‚úÖ MFA for AWS console access
- ‚úÖ CloudTrail for audit logging
- ‚úÖ Resource tagging for cost allocation

### 5. Compliance

- ‚úÖ VPC Flow Logs (90 days retention)
- ‚úÖ CloudTrail logs (1 year retention)
- ‚úÖ Intervention audit trail (90 days)
- ‚úÖ Encryption for PII data

---

## Disaster Recovery

### Backup Strategy

1. **RDS Automated Backups**
   - Daily snapshots
   - 7-day retention
   - Point-in-time recovery

2. **S3 Versioning**
   - Model versioning enabled
   - 90-day retention for old versions

3. **Configuration Backups**
   - Terraform state in S3
   - State locking with DynamoDB

### Recovery Procedures

#### Scenario 1: Database Failure

```bash
# Restore from latest snapshot
aws rds restore-db-instance-from-db-snapshot \
  --db-instance-identifier pre-delinquency-engine-db-restored \
  --db-snapshot-identifier <snapshot-id>

# Update Terraform state
terraform import aws_db_instance.postgres pre-delinquency-engine-db-restored
```

#### Scenario 2: Service Failure

```bash
# Force new deployment
aws ecs update-service \
  --cluster pre-delinquency-engine-cluster \
  --service pre-delinquency-engine-api \
  --force-new-deployment

# Scale up tasks
aws ecs update-service \
  --cluster pre-delinquency-engine-cluster \
  --service pre-delinquency-engine-api \
  --desired-count 5
```

#### Scenario 3: Region Failure

```bash
# Deploy to secondary region
cd deployment/terraform
terraform apply \
  -var="aws_region=us-west-2" \
  -var="environment=dr"
```

---

## Troubleshooting

### Common Issues

#### 1. ECS Tasks Not Starting

```bash
# Check task logs
aws ecs describe-tasks \
  --cluster pre-delinquency-engine-cluster \
  --tasks <task-id>

# Check service events
aws ecs describe-services \
  --cluster pre-delinquency-engine-cluster \
  --services pre-delinquency-engine-api
```

#### 2. High CPU/Memory Usage

```bash
# Scale up service
aws ecs update-service \
  --cluster pre-delinquency-engine-cluster \
  --service pre-delinquency-engine-api \
  --desired-count 5

# Increase task resources
# Edit task definition and update service
```

#### 3. Database Connection Issues

```bash
# Check security group rules
aws ec2 describe-security-groups \
  --group-ids <rds-sg-id>

# Test connection from ECS task
aws ecs execute-command \
  --cluster pre-delinquency-engine-cluster \
  --task <task-id> \
  --container api \
  --interactive \
  --command "/bin/bash"
```

#### 4. MSK Consumer Lag

```bash
# Check consumer group lag
aws kafka describe-cluster \
  --cluster-arn <msk-cluster-arn>

# Scale up consumer tasks
aws ecs update-service \
  --cluster pre-delinquency-engine-cluster \
  --service pre-delinquency-engine-feature-processor \
  --desired-count 5
```

---

## Cleanup

### Destroy All Resources

```bash
# Destroy Terraform resources
cd deployment/terraform
terraform destroy -var="alert_email=your-email@example.com"

# Delete ECR images
aws ecr batch-delete-image \
  --repository-name pre-delinquency-engine/api \
  --image-ids imageTag=latest

# Empty S3 buckets
aws s3 rm s3://pre-delinquency-engine-models-<account-id> --recursive
aws s3 rb s3://pre-delinquency-engine-models-<account-id>
```

---

## Next Steps

1. **Setup CI/CD Pipeline**
   - Connect GitHub repository
   - Configure CodePipeline
   - Enable automated deployments

2. **Configure Custom Domain**
   - Register domain in Route 53
   - Create SSL certificate in ACM
   - Update ALB listener

3. **Enable Advanced Monitoring**
   - Setup X-Ray tracing
   - Create custom CloudWatch dashboards
   - Configure SNS alerts

4. **Implement Blue/Green Deployment**
   - Configure ECS deployment strategies
   - Setup canary deployments
   - Implement rollback procedures

5. **Performance Optimization**
   - Enable RDS Performance Insights
   - Configure ElastiCache for session storage
   - Optimize MSK partition strategy

---

## Summary

This AWS deployment guide provides a production-ready infrastructure for the Pre-Delinquency Engine with:

‚úÖ **Scalable Architecture** - ECS Fargate with auto-scaling  
‚úÖ **High Availability** - Multi-AZ deployment  
‚úÖ **Security** - Encryption, secrets management, least privilege  
‚úÖ **Monitoring** - CloudWatch dashboards, alarms, X-Ray tracing  
‚úÖ **CI/CD** - Automated deployments with CodePipeline  
‚úÖ **Cost Optimized** - Right-sized resources, auto-scaling  
‚úÖ **Disaster Recovery** - Automated backups, multi-region support  

**Estimated Time to Deploy**: 2-3 hours  
**Monthly Cost**: $590-880 (production), $195 (development)  
**Maintenance**: Minimal with managed services

---

**Last Updated**: February 2026  
**Version**: 1.0.0  
**Status**: Production Ready
